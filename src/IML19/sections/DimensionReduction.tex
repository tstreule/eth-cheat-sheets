\section{Dimension reduction}

% ===
\emph{PCA}
\enskip $\normalcolor
\to f\!\!: \mathbb{R}^d\to\mathbb{R}^k, k\!<\!d
\hfill (\lambda_1\!\geq\!...\!\geq\!\lambda_d\!\geq\!0)$

\textbf{centered:} $\mu = \mathbb{E}(X) = \frac{1}{n} \sum_{i=1}^n x_i = 0$\\
\textbf{empir. cov.:} $\Sigma = \frac{1}{n} \sum_{i=1}^n x_ix_i^\top = \sum_{i=1}^d \lambda_i v_iv_i^\top$

$(\hat W, \hat z_1, .., \hat z_n) = \arg\min \sum_{i=1}^n \norm{Wz_i - x_i}_2^2$

\textbf{Sol.:} \highlight*{$\hat z_i = \hat W^\top x_i$}, $\hat W = (v_1\vert..\vert v_k) \in \mathbb{R}^{d\times k}$, orth.

% ===
\emph{Kernel PCA \enskip
\normalfont\sffamily$\normalcolor\to$ non-linear, feature discov.}
\textbf{Ansatz:} {\small see KLR},\enskip
\textbf{Constraint:} $\norm{w}_2 \!\!=\! \alpha^{\!\top}\!K\alpha \!=\! 1$

\textbf{Kernel PC:} $\alpha^{(1)}\!,..,\alpha^{(k)}\!\in\!\mathbb{R}^n$, \enskip
$\alpha^{(i)} \!=\! \frac{1}{\sqrt{\lambda_i}}v_i$,\\
$K = \sum_{i=1}^n \lambda_i v_i v_i^\top$, $\lambda_1\!\geq\!..\!\geq\!\lambda_d\!\geq\!0$\\
\textbf{New point}: \highlight{$\hat{z}_i = \sum_{j=1}^n\alpha_j^{(i)}k(\hat{x}_i,x_j)$}

% ===
\emph{Autoencoders:}
Find identity fct.: $x \approx f(x;\theta)$\\
$f(x;\theta) = f\ped{decode}(f\ped{encode}(x;\theta\ped{enc.});\theta\ped{dec.})$
